{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ed07b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8cb9b",
   "metadata": {},
   "source": [
    "# PyTorch Workflow Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074706b8",
   "metadata": {},
   "source": [
    "The essence of machine learning and deep learning is to take some data from the past, build an algorithm (like a neural network) to discover patterns in it and use the discovered patterns to predict the future.\n",
    "\n",
    "There are many ways to do this and many new ways are being discovered all the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d875aca4",
   "metadata": {},
   "source": [
    "## Standard Workflow\n",
    "\n",
    "In this notebook we're going to cover a standard PyTorch workflow (it can be chopped and changed as necessary but it covers the main outline of steps).\n",
    "\n",
    "![wrkflow](docs/img/01/01_a_pytorch_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b5378",
   "metadata": {},
   "source": [
    "## Classification Usacase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec56969",
   "metadata": {},
   "source": [
    "Classification, along with regression (predicting a number) is one of the most common types of machine learning problems.\n",
    "\n",
    ">NOTE: We will only cover classification example, the approach to ML for regression has the same steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a913694a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "A classification problem involves predicting whether something is one thing or another.\n",
    "\n",
    "![classification](docs/img/01/02-different-classification-problems.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ec4eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1. Data Prep\n",
    "\n",
    "I want to stress that \"data\" in machine learning can be almost anything you can imagine. A table of numbers (like a big Excel spreadsheet), images of any kind, videos (YouTube has lots of data!), audio files like songs or podcasts, protein structures, text and more.\n",
    "\n",
    "![dataprep](docs/img/01/01-machine-learning-a-game-of-two-parts.png)\n",
    "\n",
    "Machine learning is a game of two parts:\n",
    "\n",
    "1. Turn your data, whatever it is, into numbers (a representation).\n",
    "2. Pick or build a model to learn the representation as best as possible.\n",
    "\n",
    "Sometimes one and two can be done at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 1000 samples \n",
    "n_samples = 1000\n",
    "\n",
    "# Create circles\n",
    "X, y = make_circles(n_samples,\n",
    "                    noise=0.03, # a little bit of noise to the dots\n",
    "                    random_state=42) # keep random state so we get the same values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First 5 X features:\\n{X[:5]}\")\n",
    "print(f\"\\nFirst 5 y labels:\\n{y[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ed329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DataFrame of circle data\n",
    "circles = pd.DataFrame({\"X1\": X[:, 0],\n",
    "    \"X2\": X[:, 1],\n",
    "    \"label\": y\n",
    "})\n",
    "circles.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d87c3",
   "metadata": {},
   "source": [
    "It looks like each pair of X features (X1 and X2) has a label (y) value of either 0 or 1.\n",
    "\n",
    "This tells us that our problem is **binary classification** since there's only two options (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d456b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check different labels\n",
    "circles.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cd4a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with a plot\n",
    "plt.scatter(x=X[:, 0], \n",
    "            y=X[:, 1], \n",
    "            c=y, \n",
    "            cmap=plt.cm.RdYlBu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5febf9ed",
   "metadata": {},
   "source": [
    "Looks like we've got a problem to solve.\n",
    "\n",
    "Let's find out how we could build a PyTorch neural network to classify dots into red (0) or blue (1).\n",
    "\n",
    ">\n",
    "\n",
    "    Note: This dataset is often what's considered a toy problem (a problem that's used to try and test things out on) in machine learning.\n",
    "    But it represents the major key of classification, you have some kind of data represented as numerical values and you'd like to build a model that's able to classify it, in our case, separate it into red or blue dots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes of our features and labels\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first example of features and labels\n",
    "X_sample = X[0]\n",
    "y_sample = y[0]\n",
    "print(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\")\n",
    "print(f\"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565015e",
   "metadata": {},
   "source": [
    "This tells us the second dimension for X means it has two features (vector) where as y has a single feature (scalar)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8820699",
   "metadata": {},
   "source": [
    "#### Create test/train splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606497ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data into tensors\n",
    "# Otherwise this causes issues with computations later on\n",
    "import torch\n",
    "X = torch.from_numpy(X).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "# View the first five samples\n",
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7307bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, # 20% test, 80% train\n",
    "                                                    random_state=42) # make the random split reproducible\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885b73c9",
   "metadata": {},
   "source": [
    "Nice! Looks like we've now got 800 training samples and 200 testing samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ad93d",
   "metadata": {},
   "source": [
    "### 2. Building a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c8e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b00cb1b",
   "metadata": {},
   "source": [
    "1. Models in PyTorch must inherit from nn.Module. This gives you all the methods and properties that you need\n",
    "\n",
    "2. You can define layers of your neural network in the `__init__` constructor method\n",
    "\n",
    "3. Have a look at [nn.Linear()](https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html). (It is an easy way to define a feed forward layer as you don't have to manually define weights and biases -> See below, a simple regression example)\n",
    "\n",
    "4. You must define a `forward` method of your model class. This method defines how data flows through the network.\n",
    "\n",
    "![regLinear](docs/img/01/01-pytorch-linear-regression-model-with-nn-Parameter-and-nn-Linear-compared.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a22ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Construct a model class that subclasses nn.Module\n",
    "class CircleModelV0(nn.Module): # <- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use nn.Linear() for creating the model parameters\n",
    "        #  Create 2 nn.Linear layers capable of handling X and y input and output shapes\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features (X), produces 5 features\n",
    "        self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features, produces 1 feature (y)\n",
    "    \n",
    "    # Forward defines the computation in the model\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data (e.g. training/testing features)\n",
    "        # Return the output of layer_2, a single feature, the same shape as y\n",
    "        return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2bdb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model and send it to target device\n",
    "model_0 = CircleModelV0().to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0df3455",
   "metadata": {},
   "source": [
    "What's going on here?\n",
    "\n",
    "\n",
    "self.layer_1 takes 2 input features in_features=2 and produces 5 output features out_features=5.\n",
    "\n",
    "This is known as having 5 **hidden units** or **neurons**.\n",
    "\n",
    "This layer turns the input data from having 2 features to 5 features.\n",
    "\n",
    "Why do this?\n",
    "\n",
    "This allows the model to learn patterns from 5 numbers rather than just 2 numbers, *potentially* leading to better outputs.\n",
    "\n",
    "I say *potentially* because sometimes it doesn't work.\n",
    "\n",
    "The number of hidden units you can use in neural network layers is a **hyperparameter** (a value you can set yourself) and there's no set in stone value you have to use.\n",
    "\n",
    "Generally more is better but there's also such a thing as too much. The amount you choose will depend on your model type and dataset you're working with.\n",
    "\n",
    "Since our dataset is small and simple, we'll keep it small.\n",
    "\n",
    "The only rule with hidden units is that the next layer, in our case,` self.layer_2` has to take the same `in_features` as the previous layer` out_features`.\n",
    "\n",
    "That's why `self.layer_2` has `in_features=5`, it takes the `out_features=5` from `self.layer_1` and performs a linear computation on them, turning them into `out_features=1` (the same shape as y).\n",
    "\n",
    "You can visualise simple NNs in this playground: https://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1a95a1",
   "metadata": {},
   "source": [
    "#### Use nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace6d0a",
   "metadata": {},
   "source": [
    "Alternatively, you group your \"hidden layers\" in a `nn.Sequential` container, I prefer this approach as you can do a forward pass of your data with one function call. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircleModelV0(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=2, out_features=5), # takes in 2 features (X), produces 5 features\n",
    "            nn.Linear(in_features=5, out_features=1), # takes in 5 features, produces 1 feature (y)\n",
    "        )\n",
    "    \n",
    "    # Forward defines the computation in the model\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: \n",
    "        return self.net(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1240c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model_0 = CircleModelV0().to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3bc7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a145e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List named parameters \n",
    "model_0.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986a2dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the model\n",
    "with torch.no_grad(): # <- Disable gradient computations\n",
    "    untrained_preds = model_0(X_test.to(device))\n",
    "\n",
    "print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\n",
    "print(f\"Length of test samples: {len(y_test)}, Shape: {y_test.shape}\")\n",
    "print(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\")\n",
    "print(f\"\\nFirst 10 test labels:\\n{y_test[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98828fc",
   "metadata": {},
   "source": [
    "Hmm, it seems there are the same amount of predictions as there are test labels but the predictions don't look like they're in the same form or shape as the test labels.\n",
    "\n",
    "We've got a couple steps we can do to fix this, we'll see these later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6261c8",
   "metadata": {},
   "source": [
    "### Loss function and Optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bb69c2",
   "metadata": {},
   "source": [
    "For our model to update its parameters on its own, we'll need to add a few more things to our recipe.\n",
    "\n",
    "And that's a loss **function** as well as an **optimiser**.\n",
    "\n",
    "![loss-opt](docs/img/01/01-loss-optim.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1018cf",
   "metadata": {},
   "source": [
    "Let's create a loss function and an optimizer we can use to help improve our model.\n",
    "\n",
    "Depending on what kind of problem you're working on will depend on what loss function and what optimizer you use.\n",
    "\n",
    "However, there are some common values, that are known to work well such as the SGD (stochastic gradient descent) or Adam optimizer. And the MAE (mean absolute error) loss function for regression problems (predicting a number) or binary cross entropy loss function for classification problems (predicting one thing or another).\n",
    "\n",
    "![more-det](docs/img/01/01-ompit-loss-detail.png)\n",
    "\n",
    "Below is an example of a loss function used for regression and what it means -> We are trying to minimise the difference.\n",
    "\n",
    "![reg-loss](docs/img/01/01-mae-loss-annotated.png)\n",
    "\n",
    "While different promblems usually require different loss functions the same optimizer function can often be used across different problem spaces.\n",
    "\n",
    "For example, the stochastic gradient descent optimizer `(SGD, torch.optim.SGD())` can be used for a range of problems, and the same applies to the Adam optimizer `(torch.optim.Adam()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212768fe",
   "metadata": {},
   "source": [
    ">Note: Recall a loss function is what measures how wrong your model predictions are, the higher the loss, the worse your model. Also, PyTorch documentation often refers to loss functions as \"loss criterion\" or \"criterion\", these are all different ways of describing the same thing.\n",
    "\n",
    "PyTorch has two binary cross entropy implementations:\n",
    "\n",
    "1. [`torch.nn.BCELoss()`](https://docs.pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) - Creates a loss function that measures the binary cross entropy between the target (label) and input (features).\n",
    "2. [`torch.nn.BCEWithLogitsLoss()`](https://docs.pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) - This is the same as above except it has a sigmoid layer [`(nn.Sigmoid)`](https://docs.pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) built-in (we'll see what this means soon).\n",
    "\n",
    "Which one should you use?\n",
    "\n",
    "The documentation for `torch.nn.BCEWithLogitsLoss()` states that it's more numerically stable than using `torch.nn.BCELoss()` after a nn.Sigmoid layer.\n",
    "\n",
    "So generally, implementation 2 is a better option. However for advanced usage, you may want to separate the combination of `nn.Sigmoid` and `torch.nn.BCELoss()` but that is beyond the scope of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135540a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in\n",
    "loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a02fe8",
   "metadata": {},
   "source": [
    "#### Optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f92e0",
   "metadata": {},
   "source": [
    "We'll use SGD, torch.optim.SGD(params, lr) where:\n",
    "\n",
    "- `params` is the target model parameters you'd like to optimize (e.g. the `weights` and `bias` values we randomly set before).\n",
    "- `lr` is the learning rate you'd like the optimizer to update the parameters at, higher means the optimizer will try larger updates (these can sometimes be too large and the optimizer will fail to work), lower means the optimizer will try smaller updates (these can sometimes be too small and the optimizer will take too long to find the ideal values). The learning rate is considered a **hyperparameter** (because it's set by a machine learning engineer). Common starting values for the learning rate are `0.01`, `0.001`, `0.0001`, however, these can also be adjusted over time (this is called learning rate scheduling).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0648a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd8eca",
   "metadata": {},
   "source": [
    "#### Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c1385d",
   "metadata": {},
   "source": [
    "\n",
    "Now let's also create an **evaluation metric**.\n",
    "\n",
    "An evaluation metric can be used to offer another perspective on how your model is going.\n",
    "\n",
    "If a loss function measures how wrong your model is, I like to think of evaluation metrics as measuring how right it is.\n",
    "\n",
    "Of course, you could argue both of these are doing the same thing but evaluation metrics offer a different perspective.\n",
    "\n",
    "After all, when evaluating your models it's good to look at things from multiple points of view.\n",
    "\n",
    "There are several evaluation metrics that can be used for classification problems but let's start out with **accuracy**.\n",
    "\n",
    "Accuracy can be measured by dividing the total number of correct predictions over the total number of predictions.\n",
    "\n",
    "For example, a model that makes 99 correct predictions out of 100 will have an accuracy of 99%.\n",
    "\n",
    "Let's write a function to do so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe84529",
   "metadata": {},
   "source": [
    "### 3. Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd059c",
   "metadata": {},
   "source": [
    "A typical training loop will do the following:\n",
    "\n",
    "![train-loop](docs/img/01/01-train-steps.png)\n",
    "\n",
    "![ann-train](docs/img/01/01-pytorch-training-loop-annotated.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d6dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_0.train()\n",
    "\n",
    "    # 1. Forward pass (model outputs raw logits)\n",
    "    y_logits = model_0(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device \n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labls\n",
    "  \n",
    "    # 2. Calculate loss/accuracy\n",
    "    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n",
    "    #                y_train) \n",
    "    loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits\n",
    "                   y_train) \n",
    "    acc = accuracy_fn(y_true=y_train, \n",
    "                      y_pred=y_pred) \n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = model_0(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        # 2. Caculate loss/accuracy\n",
    "        test_loss = loss_fn(test_logits,\n",
    "                            y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                               y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040befe",
   "metadata": {},
   "source": [
    "What can you tell about the performance?\n",
    "\n",
    "Can you guess why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7318e2",
   "metadata": {},
   "source": [
    "### Optimising\n",
    "\n",
    "Lets see the results. Let's make a plot of our model's predictions, the data it's trying to predict on and the decision boundary it's creating for whether something is class 0 or class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67961838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_predictions, plot_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4746c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries for training and test sets\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_0, X_train, y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_0, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae5702c",
   "metadata": {},
   "source": [
    "Oh wow, it seems like we've found the cause of model's performance issue.\n",
    "\n",
    "It's currently trying to split the red and blue dots using a straight line...\n",
    "\n",
    "That explains the 50% accuracy. Since our data is circular, drawing a straight line can at best cut it down the middle.\n",
    "\n",
    "In machine learning terms, our model is **underfitting**, meaning it's not learning predictive patterns from the data.\n",
    "\n",
    "How could we improve this?\n",
    "\n",
    "Let's see what can be done to a model to improve it... Focusing specifically on the model (not the data), there are a few ways we could do this.\n",
    "\n",
    "![optim](docs/img/01/01-model-optim.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcdcb2b",
   "metadata": {},
   "source": [
    "## The missing piece: non-linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b2b84",
   "metadata": {},
   "source": [
    "Let's see the raw data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4672f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with a plot\n",
    "plt.scatter(x=X[:, 0], \n",
    "            y=X[:, 1], \n",
    "            c=y, \n",
    "            cmap=plt.cm.RdYlBu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a989252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with non-linear activation function\n",
    "from torch import nn\n",
    "class CircleModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=2, out_features=10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=10, out_features=10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=10, out_features=1),\n",
    "            # nn.ReLU() # <- Should I include this?\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "      # Intersperse the ReLU activation function between layers\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f95a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2 = CircleModelV2().to(device)\n",
    "print(model_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d1287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss and optimizer \n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model_v2.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3046b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "torch.manual_seed(42)\n",
    "epochs = 1400\n",
    "\n",
    "# Put all data on target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1. Forward pass\n",
    "    y_logits = model_v2(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> prediction probabilities -> prediction labels\n",
    "    \n",
    "    # 2. Calculate loss and accuracy\n",
    "    loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss calculates loss using logits\n",
    "    acc = accuracy_fn(y_true=y_train, \n",
    "                      y_pred=y_pred)\n",
    "    \n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backward\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_v2.eval()\n",
    "    with torch.inference_mode():\n",
    "      # 1. Forward pass\n",
    "      test_logits = model_v2(X_test).squeeze()\n",
    "      test_pred = torch.round(torch.sigmoid(test_logits)) # logits -> prediction probabilities -> prediction labels\n",
    "      # 2. Calculate loss and accuracy\n",
    "      test_loss = loss_fn(test_logits, y_test)\n",
    "      test_acc = accuracy_fn(y_true=y_test,\n",
    "                             y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77663c53",
   "metadata": {},
   "source": [
    "https://playground.tensorflow.org/ -> Lets see if we add non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba42f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries for training and test sets\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original model with linear activation\")\n",
    "plot_decision_boundary(model_0, X_train, y_train) # model_1 = no non-linearity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"New model with non-linear activation\")\n",
    "plot_decision_boundary(model_v2, X_test, y_test) # model_3 = has non-linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da700dfe",
   "metadata": {},
   "source": [
    "### Try Multi-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa592a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters for data creation\n",
    "NUM_CLASSES = 4\n",
    "NUM_FEATURES = 2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# 1. Create multi-class data\n",
    "X_blob, y_blob = make_blobs(n_samples=1000, # type: ignore\n",
    "    n_features=NUM_FEATURES, # X features\n",
    "    centers=NUM_CLASSES, # y labels \n",
    "    cluster_std=1.5, # give the clusters a little shake up (try changing this to 1.0, the default)\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# 2. Turn data into tensors\n",
    "X_blob = torch.from_numpy(X_blob).type(torch.float)\n",
    "y_blob = torch.from_numpy(y_blob).type(torch.LongTensor) # type: ignore\n",
    "print(X_blob[:5], y_blob[:5])\n",
    "\n",
    "# 3. Split into train and test sets\n",
    "X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,\n",
    "    y_blob,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# 4. Plot data\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu); # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da654e",
   "metadata": {},
   "source": [
    "We've created a few models in PyTorch so far.\n",
    "\n",
    "You might also be starting to get an idea of how flexible neural networks are.\n",
    "\n",
    "How about we build one similar to `model_3` but this is still capable of handling multi-class data?\n",
    "\n",
    "To do so, let's create a subclass of `nn.Module` that takes in three **hyperparameters**:\n",
    "\n",
    "- `input_features` - the number of X features coming into the model.\n",
    "- `output_features` - the ideal numbers of output features we'd like (this will be equivalent to NUM_CLASSES or the number of classes in your multi-class classification problem).\n",
    "- `hidden_units` - the number of hidden neurons we'd like each hidden layer to use.\n",
    "\n",
    "Since we're putting things together, let's setup some device agnostic code (we don't have to do this again in the same notebook, it's only a reminder).\n",
    "\n",
    "Then we'll create the model class using the hyperparameters above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e911161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "class BlobModel(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_units=8):\n",
    "        \"\"\"Initializes all required hyperparameters for a multi-class classification model.\n",
    "\n",
    "        Args:\n",
    "            input_features (int): Number of input features to the model.\n",
    "            out_features (int): Number of output features of the model\n",
    "              (how many classes there are).\n",
    "            hidden_units (int): Number of hidden units between layers, default 8.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
    "            # nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            # nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_features), # how many classes are there?\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45426fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of BlobModel and send it to the target device\n",
    "blob_model = BlobModel(input_features=NUM_FEATURES, \n",
    "                    output_features=NUM_CLASSES, \n",
    "                    hidden_units=8).to(device)\n",
    "blob_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f8b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(blob_model.parameters(), \n",
    "                            lr=0.1) # exercise: try changing the learning rate here and seeing what happens to the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3254b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a single forward pass on the data (we'll need to put it to the target device for it to work)\n",
    "blob_model(X_blob_train.to(device))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee91cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many elements in a single prediction sample?\n",
    "blob_model(X_blob_train.to(device))[0].shape, NUM_CLASSES "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a1c0d2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Wonderful, our model is predicting one value for each class that we have.\n",
    "\n",
    "Do you remember what the raw outputs of our model are called?\n",
    "\n",
    "Hint: it rhymes with \"frog splits\" (no animals were harmed in the creation of these materials).\n",
    "\n",
    "If you guessed logits, you'd be correct.\n",
    "\n",
    "So right now our model is outputing logits but what if we wanted to figure out exactly which label is was giving the sample?\n",
    "\n",
    "As in, how do we go from logits -> prediction probabilities -> prediction labels just like we did with the binary classification problem?\n",
    "\n",
    "That's where the softmax activation function comes into play.\n",
    "\n",
    "The softmax function calculates the probability of each prediction class being the actual predicted class compared to all other possible classes.\n",
    "\n",
    "If this doesn't make sense, let's see in code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb74f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction logits with model\n",
    "y_logits = blob_model(X_blob_test.to(device))\n",
    "\n",
    "# Perform softmax calculation on logits across dimension 1 to get prediction probabilities\n",
    "y_pred_probs = torch.softmax(y_logits, dim=1) \n",
    "print(y_logits[:5])\n",
    "print(y_pred_probs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f18899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the first sample output of the softmax activation function \n",
    "torch.sum(y_pred_probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9570b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Put data to target device\n",
    "X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\n",
    "X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    blob_model.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits = blob_model(X_blob_train) # model outputs raw logits \n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
    "    # print(y_logits)\n",
    "    # 2. Calculate loss and accuracy\n",
    "    loss = loss_fn(y_logits, y_blob_train) \n",
    "    acc = accuracy_fn(y_true=y_blob_train,\n",
    "                      y_pred=y_pred)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    blob_model.eval()\n",
    "    with torch.inference_mode():\n",
    "      # 1. Forward pass\n",
    "      test_logits = blob_model(X_blob_test)\n",
    "      test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "      # 2. Calculate test loss and accuracy\n",
    "      test_loss = loss_fn(test_logits, y_blob_test)\n",
    "      test_acc = accuracy_fn(y_true=y_blob_test,\n",
    "                             y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d067bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(blob_model, X_blob_train, y_blob_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(blob_model, X_blob_test, y_blob_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e49e65",
   "metadata": {},
   "source": [
    "### More evaluation metrics\n",
    "\n",
    "![eval-mets](docs/img/01/01-eval-mets.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0cf59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "blob_model.eval()\n",
    "blob_model.to(device)\n",
    "with torch.inference_mode():\n",
    "    y_logits = blob_model(X_blob_test.to(device))\n",
    "\n",
    "# Turn predicted logits in prediction probabilities\n",
    "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
    "\n",
    "# Turn prediction probabilities into prediction labels\n",
    "y_preds = y_pred_probs.argmax(dim=1)\n",
    "\n",
    "# Compare first 10 model preds and test labels\n",
    "print(f\"Predictions: {y_preds[:10]}\\nLabels: {y_blob_test[:10]}\")\n",
    "print(f\"Test accuracy: {accuracy_fn(y_true=y_blob_test, y_pred=y_preds)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49955fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy, F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e5e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchmetrics_accuracy = Accuracy(task='multiclass', num_classes=4).to(device)\n",
    "\n",
    "# Calculate accuracy\n",
    "torchmetrics_accuracy(y_preds, y_blob_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a74b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchmetrics_f1 = F1Score(task='multiclass', num_classes=4).to(device)\n",
    "\n",
    "# Calculate f1\n",
    "torchmetrics_f1(y_preds, y_blob_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920314f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
